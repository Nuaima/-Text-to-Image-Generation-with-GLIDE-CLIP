# ğŸ–¼ï¸ Text-to-Image Generation with GLIDE + CLIP

This repository contains an implementation of **GLIDE** (Guided Language to Image Diffusion for Generation and Editing) with **CLIP-based guidance** for text-to-image generation.  
It takes a **text prompt** and produces **high-resolution (256Ã—256) images** by first generating a low-resolution sample and then upscaling it with an upsampler model.

---

## ğŸš€ Features
- Generate images from any text description.
- Uses **CLIP guidance** to align images with the text prompt.
- Supports **GPU acceleration (CUDA)** for faster sampling.
- Produces **64Ã—64 â†’ 256Ã—256 upsampled images**.
- Easy to modify for different prompts, batch sizes, and sampling steps.

---

## ğŸ“‚ Repository Structure
ğŸ“¦ glide-text2im-project/
â”œâ”€â”€ ğŸ“„ clip_guided_notebook.ipynb # Main Colab notebook implementation
â”œâ”€â”€ ğŸ“„ README.md # Project documentation
â””â”€â”€ ğŸ“ samples/ # Generated images 

---

### ğŸ› ï¸ Technologies Used
- **Programming Language:** Python 3  
- **Libraries & Frameworks:**
  - **PyTorch** â†’ Deep learning framework for model loading & inference  
  - **GLIDE (OpenAI Glide-Text2IM)** â†’ Text-to-Image generation model  
  - **HuggingFace Transformers / Diffusers** â†’ For handling pretrained models and pipelines  
  - **PIL (Python Imaging Library)** â†’ Image processing and display  
  - **IPython.display** â†’ Displaying images in Jupyter/Colab  
- **Tools:**
  - Google Colab / Jupyter Notebook for execution  
  - GitHub for version control and project hosting  
  - CUDA (if GPU available) for faster model inference  
---
